{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n",
    "1. split main task into stages\n",
    "2. epoch motor planning (-500, 500) around target onset\n",
    "3. epoch motor execution (-500, 700) around movement onset\n",
    "4. time-frequency for individual subjects + group analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "from utils import check_paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLANNING / EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. EPOCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create planning / execution epochs for subjects\n",
    "eeg_data_dir = 'D:\\\\BonoKat\\\\research project\\\\# study 1\\\\eeg_data\\\\set'\n",
    "group = 'O'\n",
    "task = '_MAIN' # ['_BL', '_MAIN']\n",
    "task_stage = '_plan' # '_plan' for PLANNING or '_go' for EXECUTION\n",
    "\n",
    "subs_dir = os.path.join(eeg_data_dir, group)\n",
    "figs_dir = os.path.join(eeg_data_dir, 'figures', group, 'epochs', task)\n",
    "check_paths(figs_dir)\n",
    "\n",
    "# trial infor dictionary\n",
    "trial_num_data = {'sub': [], 'task_stage': [], 'block_name': [], 'trial_num': []}\n",
    "\n",
    "# metadata for each epoch shall include events from the range: [-0.5, 0.7] s,\n",
    "# i.e. starting with stimulus onset and expanding beyond the end of the epoch\n",
    "if task_stage == '_plan':\n",
    "    metadata_tmin, metadata_tmax = -0.5, 0.5 # [-0.5, 0.7] for _go or [-0.5, 0.5] for _plan\n",
    "    # events of interest\n",
    "    row_events = ['target_on'] # ['target_on'] or ['go_on']\n",
    "    # timing of the epochs\n",
    "    epochs_tmin, epochs_tmax = -0.5, 0.5  # epochs range: [-0.5, 0.7] for _go or [-0.5, 0.5] for _plan\n",
    "\n",
    "else:\n",
    "    metadata_tmin, metadata_tmax = -0.5, 0.7 # [-0.5, 0.7] for _go or [-0.5, 0.5] for _plan\n",
    "    # events of interest\n",
    "    row_events = ['go_on'] # ['target_on'] or ['go_on']\n",
    "    # timing of the epochs\n",
    "    epochs_tmin, epochs_tmax = -0.5, 0.7  # epochs range: [-0.5, 0.7] for _go or [-0.5, 0.5] for _plan\n",
    "\n",
    "baseline_epo = (epochs_tmin, epochs_tmax) # baseline is the average of the epoch\n",
    "\n",
    "\n",
    "for sub_name in os.listdir(subs_dir): # os.listdir(subs_dir) OR ['s1_pac_sub00'] # EXCLUDED sub19 - no baseline trigger\n",
    "\n",
    "    print(f'Creating epochs for {task} task in {sub_name}...')\n",
    "\n",
    "    preproc_dir = os.path.join(subs_dir, sub_name, 'preproc')\n",
    "    filt_dir = os.path.join(subs_dir, sub_name, 'preproc', 'filt')\n",
    "    analysis_dir = os.path.join(preproc_dir, 'analysis')\n",
    "\n",
    "    eeg_data_path = os.path.join(analysis_dir, f'{sub_name}{task}_reconst.fif')\n",
    "    raw = mne.io.read_raw_fif(eeg_data_path, preload=True)\n",
    "    sf = raw.info['sfreq'] # sampling frequency of data\n",
    "\n",
    "    # Open events from pickle file\n",
    "    with open(os.path.join(filt_dir, f'{sub_name}{task}_events.pkl'), 'rb') as pickle_file:\n",
    "        events_raw = pickle.load(pickle_file)\n",
    "\n",
    "    # check if event time samples are unique\n",
    "    for i in range(len(events_raw[0]) - 1):\n",
    "        if events_raw[0][i, 0] == events_raw[0][i+1, 0]:\n",
    "            print(f'Time sample for events_raw {events_raw[0][i, 2]} and {events_raw[0][i+1, 2]} in trials {i}-{i+1} are not unique!')\n",
    "            events_raw[0][i+1, 0] = events_raw[0][i, 0] + 1\n",
    "\n",
    "\n",
    "    # extract time stamps (in samples) from the events_raw[0] array for key in events_raw[1] dict\n",
    "    bl_start_sample = events_raw[0][events_raw[0][:, 2] == events_raw[1]['baseline']] \n",
    "    adapt_start_sample = events_raw[0][events_raw[0][:, 2] == events_raw[1]['adapt1']]\n",
    "    adapt_finish_sample = events_raw[0][events_raw[0][:, 2] == events_raw[1]['postadapt']]\n",
    "    \n",
    "    # convert time stamps from sample to seconds\n",
    "    bl_start_time = bl_start_sample[0, 0] / sf\n",
    "    adapt_start_time = adapt_start_sample[0, 0] / sf\n",
    "    adapt_finish_time = adapt_finish_sample[0, 0] / sf\n",
    "\n",
    "    print(f'''baseline block: {bl_start_time}-{adapt_start_time} sec ~ {(adapt_start_time-bl_start_time)/60:.2f} min\n",
    "    adaptation block: {adapt_start_time}-{adapt_finish_time} sec ~ {(adapt_finish_time-adapt_start_time)/60:.2f} min''')\n",
    "\n",
    "    # crop raw file into segments: baseline block and adaptation block\n",
    "    bl_raw = raw.copy().crop(tmin=(bl_start_time-1), tmax=(adapt_start_time+1)) # add and subtract 1 to catch start and end events\n",
    "    adapt_raw = raw.copy().crop(tmin=(adapt_start_time-1), tmax=(adapt_finish_time+1))\n",
    "\n",
    "    for block_raw in [bl_raw, adapt_raw]:\n",
    "\n",
    "        if block_raw == bl_raw:\n",
    "            block_name = '_baseline'\n",
    "        else:\n",
    "            block_name = '_adaptation'\n",
    "\n",
    "        # auto-create metadata:\n",
    "        # this also returns a new events array and an event_id dictionary. we'll see\n",
    "        # later why this is important\n",
    "        metadata, events, event_id = mne.epochs.make_metadata(\n",
    "            events=events_raw[0],\n",
    "            event_id=events_raw[1],\n",
    "            tmin=metadata_tmin,\n",
    "            tmax=metadata_tmax,\n",
    "            sfreq=sf,\n",
    "            row_events=row_events\n",
    "        )\n",
    "\n",
    "        epochs = mne.Epochs(\n",
    "            raw=block_raw,\n",
    "            tmin=epochs_tmin,\n",
    "            tmax=epochs_tmax,\n",
    "            events=events,\n",
    "            event_id=event_id,\n",
    "            baseline=baseline_epo,\n",
    "            detrend=None,\n",
    "            metadata=metadata,\n",
    "            reject_by_annotation=True,\n",
    "            preload=True,\n",
    "        )\n",
    "\n",
    "        if task_stage == '_plan':\n",
    "            epochs = epochs[\"trial_start.isna() & go_on.isna() & bad_early.isna()\"]\n",
    "        \n",
    "        else:\n",
    "            if 'bad_late' in events_raw[1]:\n",
    "                epochs = epochs[\"trial_start.isna() & bad_early.isna() & bad_late.isna()\"]\n",
    "            else:\n",
    "                epochs = epochs[\"trial_start.isna() & bad_early.isna()\"]\n",
    "\n",
    "        print(f'TOTAL NUMBER OF EPOCHS: {len(epochs)}')\n",
    "\n",
    "        # Append data to the dictionary\n",
    "        trial_num_data['sub'].append(sub_name)\n",
    "        trial_num_data['task_stage'].append(task_stage)\n",
    "        trial_num_data['block_name'].append(block_name)\n",
    "        trial_num_data['trial_num'].append(len(epochs))\n",
    "\n",
    "        # Save the epochs\n",
    "        epochs.save(os.path.join(analysis_dir, f\"{sub_name}{task}_epochs{task_stage}{block_name}-epo.fif\"), overwrite=True)\n",
    "        print(f'Epochs for {task}_{block_name} in {sub_name} saved SUCCESSFULLY')\n",
    "\n",
    "        # Plot ERP\n",
    "        fig_erp = epochs.average().plot(gfp=True, spatial_colors=True)\n",
    "        # Save the ERP plot\n",
    "        fig_erp.savefig(os.path.join(figs_dir, f\"{sub_name}{task}{task_stage}{block_name}_erp_plot.png\"), dpi=300)\n",
    "\n",
    "\n",
    "        spectrum = epochs.compute_psd()\n",
    "        bands = {'Theta (4-8 Hz)': (4, 8),\n",
    "                'Alpha (8-12 Hz)': (8, 12),\n",
    "                'Beta (12-30 Hz)': (12, 30),\n",
    "                'Gamma (30-50 Hz)': (30, 50),\n",
    "                'High gamma (50-80 Hz)': (50, 80)}\n",
    "\n",
    "        fig_psd = spectrum.plot_topomap(bands=bands, vlim=\"joint\", normalize=True)\n",
    "        fig_psd.savefig(os.path.join(figs_dir, f\"{sub_name}{task}{task_stage}{block_name}_psd_topomap.png\"), dpi=300)\n",
    "\n",
    "        print(f'Figures for {sub_name} saved SUCCESSFULLY')\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "#save the trial number data to a CSV file\n",
    "trial_num_df = pd.DataFrame(trial_num_data)\n",
    "trial_num_df.to_csv(os.path.join(figs_dir, f\"{task[1:]}{task_stage}_TRIAL_NUM.csv\"), index=False)\n",
    "print(f'TRIAL NUMBER saved SUCCESSFULLY to {os.path.join(figs_dir, f\"{task[1:]}{task_stage}_TRIAL_NUM.csv\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
