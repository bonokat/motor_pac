{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0f7673",
   "metadata": {},
   "source": [
    "## MAIN SOURCE-LEVEL PAC AND STATS\n",
    "\n",
    "**This script:**\n",
    "1. Creates theta-gamma PAC comodulograms for condition (all vertices) for each subject and saves PAC data as numpy array\n",
    "2. Runs cluster-besed permutation test on PAC data\n",
    "\n",
    "**OUTCOME: PAC estimates for all vertices for each subject and statistical assessment of the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d84db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "from utils import check_paths\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import joblib\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from pactools import Comodulogram, REFERENCES, raw_to_mask\n",
    "\n",
    "from mne.channels.layout import find_layout\n",
    "from functools import partial\n",
    "from mne.defaults import _handle_default\n",
    "\n",
    "from mne.viz.topo import _erfimage_imshow_unified, _plot_topo\n",
    "\n",
    "from mne.viz.utils import (\n",
    "    _setup_vmin_vmax,\n",
    "    add_background_image\n",
    ")\n",
    "from collections import namedtuple\n",
    "\n",
    "from mne.stats import permutation_cluster_1samp_test\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd96ca",
   "metadata": {},
   "source": [
    "1. PAC analysis per condition per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf80ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data_dir = 'D:\\\\BonoKat\\\\research project\\\\# study 1\\\\eeg_data\\\\set'\n",
    "group = 'Y'\n",
    "subs = os.listdir(os.path.join(eeg_data_dir, group))\n",
    "tasks = ['_MAIN'] # ['_BL', '_MAIN']\n",
    "task_stages = ['_plan', '_go'] # ['_plan', '_go']\n",
    "block_names = ['_baseline', '_adaptation'] # ['_baseline', '_adaptation']\n",
    "\n",
    "theta_range = np.linspace(4, 8, 20)  # Phase: 4-8 Hz\n",
    "gamma_range = np.linspace(30, 80, 20)  # Amplitude: 30-80 Hz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84879b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_name in subs:\n",
    "    print(f\"Processing subject: {sub_name}\")\n",
    "    analysis_dir = os.path.join(eeg_data_dir, group, sub_name, 'preproc', 'analysis')\n",
    "    pac_save_path = os.path.join(analysis_dir, 'source', 'PAC')\n",
    "    check_paths(pac_save_path)\n",
    "\n",
    "    for task in tasks:\n",
    "        for task_stage in task_stages: # task_stages #############################\n",
    "            tmin = 0.0\n",
    "            tmax = [0.495 if task_stage == '_plan' else 0.695][0] # 0.495 for '_plan', 0.695 for '_go'\n",
    "            if task == '_MAIN':\n",
    "                for block_name in block_names: # block_names #########################\n",
    "                        stcs_path = os.path.join(analysis_dir, 'source', 'morphed_stcs', task, task_stage, block_name) \n",
    "\n",
    "                        stcs = []\n",
    "                        stcs_data = []\n",
    "\n",
    "                        for stc_file in os.listdir(stcs_path):\n",
    "                            if stc_file.endswith('-rh.stc'): # MNE will load both hemispheres anyway\n",
    "                                stc_path = os.path.join(stcs_path, stc_file)\n",
    "                                stc = mne.read_source_estimate(stc_path, subject=sub_name)\n",
    "                                stc.crop(tmin=tmin, tmax=tmax)\n",
    "                                stcs.append(stc)\n",
    "                                stcs_data.append(stc.data)\n",
    "\n",
    "                        source_array = np.stack(stcs_data, axis=0)\n",
    "                        print(source_array.shape)  # (epochs x vertices x time)\n",
    "\n",
    "                        times = stcs[0].times\n",
    "\n",
    "                        #Estimate PAC\n",
    "                        estimator = Comodulogram(\n",
    "                            fs=stcs[0].sfreq,\n",
    "                            low_fq_range=theta_range,  # Phase frequencies (theta)\n",
    "                            high_fq_range=gamma_range, # Amplitude frequencies (gamma)\n",
    "                            method='tort',\n",
    "                            progress_bar=True\n",
    "                            )\n",
    "\n",
    "                        pac_results = np.empty(\n",
    "                            (len(theta_range), source_array.shape[1], len(gamma_range))\n",
    "                        )\n",
    "\n",
    "                        for i in range(source_array.shape[1]):\n",
    "                            print(f\"Processing source {i+1}/{source_array.shape[1]}\")\n",
    "\n",
    "                            data_flat = np.reshape(source_array[:, i], -1)[None, :]\n",
    "                            pac = estimator.fit(\n",
    "                                    data_flat,\n",
    "                                    data_flat,\n",
    "                                )\n",
    "                            pac_results[:, i] = pac.comod_\n",
    "\n",
    "                            # if i in range(10):\n",
    "                            #     # Convert the plot to a Plotly figure (if supported)\n",
    "                            #     fig = pac.plot(tight_layout=False, cmap='magma')\n",
    "                            #     # Add a title\n",
    "                            #     plt.title(f\"PAC MI {sub_name[-5:]} - source={i}: {task}{task_stage}{block_name}\")\n",
    "\n",
    "                            #     # Save the plot\n",
    "                            #     plt.show()\n",
    "\n",
    "                        np.save(os.path.join(pac_save_path, f\"PAC_MI_SOURCE_{sub_name[-5:]}{task}{task_stage}{block_name}.npy\"), pac_results)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba99000",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af42dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first 10 vertices of the first epoch\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.plot(times, source_array[0, i], label=f'Vertex {i}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e0ab4",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db0510",
   "metadata": {},
   "source": [
    "# STATISTICS\n",
    "**Cluster-based permutation test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360860e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data_dir = 'D:\\\\BonoKat\\\\research project\\\\# study 1\\\\eeg_data\\\\set'\n",
    "\n",
    "groups = ['Y']\n",
    "task = '_MAIN' # ['_BL', '_MAIN']\n",
    "task_stages = ['_plan', '_go']\n",
    "block_names = ['_baseline', '_adaptation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe8715",
   "metadata": {},
   "source": [
    "**STATS FOR DATA AVERAGED ACROSS PHASE AND AMPLITUDE FREQUENCIES**\n",
    "\n",
    "**PER CONDITION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd9587",
   "metadata": {},
   "source": [
    "*Looking into imputation of NaN values (optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33106e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the sparsity pattern of the adjacency matrix\n",
    "source_adjacency = mne.spatial_src_adjacency(src)\n",
    "adj = source_adjacency.tocsr()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.spy(adj, markersize=0.5)\n",
    "plt.title(\"Sparsity pattern of CSR matrix\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.spy(source_adjacency, markersize=0.5)\n",
    "plt.title(\"Sparsity pattern of COO matrix\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 vertices with NaN values.\n",
      "NaN imputation complete.\n"
     ]
    }
   ],
   "source": [
    "### NAN Imputation for PAC Matrices ###\n",
    "# Step 0: Ensure adjacency is CSR format for fast indexing\n",
    "adj = source_adjacency.tocsr()  # your source adjacency matrix\n",
    "pac_imputed = pac_t.copy()  # to preserve original\n",
    "\n",
    "# Step 1: Detect vertices with any NaN in their 20×20 PAC\n",
    "nan_mask = np.isnan(pac_t).any(axis=(1, 2))  # shape (5124,)\n",
    "\n",
    "print(f\"Found {nan_mask.sum()} vertices with NaN values.\")\n",
    "\n",
    "# Step 2: Impute NaNs from neighbors\n",
    "for vtx in np.where(nan_mask)[0]:\n",
    "    neighbors = adj[[vtx]].indices\n",
    "    valid_neighbors = [n for n in neighbors if not nan_mask[n]]\n",
    "\n",
    "    if valid_neighbors:\n",
    "        # Average PAC matrices from neighbors\n",
    "        pac_imputed[vtx] = np.nanmean(pac_t[valid_neighbors], axis=0)\n",
    "    else:\n",
    "        # Fallback: no valid neighbors, fill with global PAC mean\n",
    "        pac_imputed[vtx] = np.nanmean(pac_t, axis=0)\n",
    "\n",
    "print(\"NaN imputation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220828ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_pac_sub01\n",
      "Total NaN values in pac_t: 2000\n",
      "Number of vertices with all-NaN PAC: 5\n",
      "Indices: [2431 3457 4029 4454 4562]\n",
      "s1_pac_sub07\n",
      "Total NaN values in pac_t: 1200\n",
      "Number of vertices with all-NaN PAC: 3\n",
      "Indices: [1648 4345 4968]\n",
      "s1_pac_sub10\n",
      "Total NaN values in pac_t: 1200\n",
      "Number of vertices with all-NaN PAC: 3\n",
      "Indices: [ 161  350 2431]\n",
      "s1_pac_sub11\n",
      "Total NaN values in pac_t: 5200\n",
      "Number of vertices with all-NaN PAC: 13\n",
      "Indices: [ 332 1360 1433 2359 2602 2956 3183 3340 3364 3384 3394 3570 5069]\n",
      "s1_pac_sub22\n",
      "Total NaN values in pac_t: 3600\n",
      "Number of vertices with all-NaN PAC: 9\n",
      "Indices: [  70 1457 1696 1993 2197 3327 3391 4562 4863]\n",
      "s1_pac_sub24\n",
      "Total NaN values in pac_t: 6800\n",
      "Number of vertices with all-NaN PAC: 17\n",
      "Indices: [ 341  956 1330 1331 1425 1738 1878 1990 2259 2431 2859 2922 3114 4035\n",
      " 4233 4699 4865]\n",
      "s1_pac_sub26\n",
      "Total NaN values in pac_t: 6000\n",
      "Number of vertices with all-NaN PAC: 15\n",
      "Indices: [ 678 1736 2361 2622 2835 3703 3742 3777 3783 3787 4205 4437 4762 4780\n",
      " 5086]\n",
      "s1_pac_sub29\n",
      "Total NaN values in pac_t: 6800\n",
      "Number of vertices with all-NaN PAC: 17\n",
      "Indices: [  25   70  445  577  610  826  830 1781 1997 1998 2198 2199 2400 2526\n",
      " 3969 4078 4991]\n",
      "s1_pac_sub32\n",
      "Total NaN values in pac_t: 3600\n",
      "Number of vertices with all-NaN PAC: 9\n",
      "Indices: [ 303  436  442 2835 3395 3940 3951 4228 5096]\n"
     ]
    }
   ],
   "source": [
    "# Find the indices of vertices with all-NaN PAC matrices\n",
    "for sub_name in subs[:9]:\n",
    "\n",
    "    print(sub_name)\n",
    "    sub_dir = os.path.join(eeg_data_dir, group, sub_name)\n",
    "    pac_dir = os.path.join(sub_dir, 'preproc', 'analysis', 'source', 'PAC')\n",
    "\n",
    "    # Load PAC data\n",
    "    pac = np.load(os.path.join(pac_dir, f\"PAC_MI_SOURCE_{sub_name[-5:]}{task}{task_stage}{block_name}.npy\"))\n",
    "    pac_t = np.transpose(pac, (1, 0, 2))\n",
    "    nan_count = np.isnan(pac_t).sum()\n",
    "    print(f\"Total NaN values in pac_t: {nan_count}\")\n",
    "\n",
    "    # Create a boolean mask for vertices with all-NaN PAC matrices\n",
    "    all_nan_vertices = np.isnan(pac_t).all(axis=(1, 2))  # shape: (5124,)\n",
    "\n",
    "    # Get indices of those vertices\n",
    "    nan_vertex_indices = np.where(all_nan_vertices)[0]\n",
    "\n",
    "    # Count them\n",
    "    print(f\"Number of vertices with all-NaN PAC: {len(nan_vertex_indices)}\")\n",
    "    print(f\"Indices: {nan_vertex_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d859b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 5124\n",
      "adjacency shape: (5124, 5124)\n",
      "Processing Y group, _MAIN task, _plan stage, _baseline block...\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub01 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub07 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub10 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub11 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub22 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub24 complete.\n",
      "Found 15 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub26 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub29 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub32 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub33 complete.\n",
      "Found 2 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub38 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub43 complete.\n",
      "Found 1 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub52 complete.\n",
      "Found 6 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub58 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub59 complete.\n",
      "Found 26 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub61 complete.\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub63 complete.\n",
      "Found 12 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub64 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub66 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub67 complete.\n",
      "Found 7 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub71 complete.\n",
      "Found 10 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub76 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub77 complete.\n",
      "PAC array shape: (23, 5124, 20, 20)\n",
      "z-scored PAC array shape: (23, 5124, 20, 20)\n",
      "stat_fun(H1): min=-5.325882479017227 max=3.7506704340039914\n",
      "Running initial clustering …\n",
      "Found 66 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aede7f19c746ca90e0ac16d1159465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 1 cluster to exclude from subsequent iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98caff41c8419da3777fe87a516607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #2 found 0 additional clusters to exclude from subsequent iterations\n",
      "t_thresh = 2.818756060596369\n",
      "T_obs_mean = -0.10162464930701287\n",
      "cluster_p_values = [1.     0.9999 0.9614 1.     1.     0.8425 1.     0.9999 0.9585 0.6849\n",
      " 0.0248 0.8041 1.     0.6853 0.9148 0.945  1.     0.8051 1.     0.6949\n",
      " 0.9636 1.     1.     0.8644 0.9998 0.6903 1.     1.     1.     1.\n",
      " 1.     0.8573 0.9807 1.     0.4921 1.     1.     0.8854 0.9833 1.\n",
      " 1.     0.9307 1.     0.2    0.3442 1.     0.9978 1.     1.     0.9983\n",
      " 1.     0.8935 1.     0.8322 1.     0.9825 1.     0.9741 0.8386 0.9081\n",
      " 0.9974 1.     1.     0.9857 1.     1.    ]\n",
      "Condition _plan_baseline: Found 1 significant clusters out of 66 total clusters.\n",
      "Processing Y group, _MAIN task, _plan stage, _adaptation block...\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub01 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub07 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub10 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub11 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub22 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub24 complete.\n",
      "Found 15 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub26 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub29 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub32 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub33 complete.\n",
      "Found 2 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub38 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub43 complete.\n",
      "Found 1 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub52 complete.\n",
      "Found 6 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub58 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub59 complete.\n",
      "Found 26 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub61 complete.\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub63 complete.\n",
      "Found 12 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub64 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub66 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub67 complete.\n",
      "Found 7 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub71 complete.\n",
      "Found 10 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub76 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub77 complete.\n",
      "PAC array shape: (23, 5124, 20, 20)\n",
      "z-scored PAC array shape: (23, 5124, 20, 20)\n",
      "stat_fun(H1): min=-6.137108709428028 max=2.1230060029489404\n",
      "Running initial clustering …\n",
      "Found 85 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57130fe8ecac4ee0a72000b21b64644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "t_thresh = 2.818756060596369\n",
      "T_obs_mean = -0.2586599887322655\n",
      "cluster_p_values = [0.8063 0.7024 0.9699 0.8826 0.7997 0.9966 0.8775 0.3162 0.2186 0.6029\n",
      " 0.7018 0.9957 0.78   0.8888 0.6846 0.4211 0.8821 0.9328 0.9715 0.6804\n",
      " 0.9094 0.654  0.6659 0.6713 0.7502 0.6147 0.9741 0.6517 0.9832 0.8492\n",
      " 0.8596 0.969  0.9477 0.7944 0.4129 0.9076 0.7979 0.7167 0.868  0.8948\n",
      " 0.5262 0.4529 0.1771 0.2947 0.6426 0.9699 0.0782 0.9896 0.9624 0.9142\n",
      " 0.3017 0.9984 0.7712 0.9098 0.8594 0.9328 0.4873 0.2923 0.2746 0.8566\n",
      " 0.6092 0.3207 0.5196 0.8169 0.8645 0.9604 0.9571 0.657  0.9893 0.9969\n",
      " 0.7926 0.4289 0.4508 0.1916 0.9283 0.3159 0.7159 0.6031 0.9952 0.9832\n",
      " 0.7865 0.8263 0.984  0.7753 0.7383]\n",
      "Condition _plan_adaptation: Found 0 significant clusters out of 85 total clusters.\n",
      "Processing Y group, _MAIN task, _go stage, _baseline block...\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub01 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub07 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub10 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub11 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub22 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub24 complete.\n",
      "Found 15 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub26 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub29 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub32 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub33 complete.\n",
      "Found 2 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub38 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub43 complete.\n",
      "Found 1 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub52 complete.\n",
      "Found 6 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub58 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub59 complete.\n",
      "Found 26 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub61 complete.\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub63 complete.\n",
      "Found 12 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub64 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub66 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub67 complete.\n",
      "Found 7 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub71 complete.\n",
      "Found 10 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub76 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub77 complete.\n",
      "PAC array shape: (23, 5124, 20, 20)\n",
      "z-scored PAC array shape: (23, 5124, 20, 20)\n",
      "stat_fun(H1): min=-5.150275800693619 max=2.996188680121844\n",
      "Running initial clustering …\n",
      "Found 60 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda4323905ad4729a5de5a733077d385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "t_thresh = 2.818756060596369\n",
      "T_obs_mean = -0.15067282161849233\n",
      "cluster_p_values = [1.     1.     0.855  0.3057 0.4529 0.9997 0.094  0.9863 1.     0.3034\n",
      " 1.     0.9957 1.     1.     1.     0.9166 0.626  1.     0.9427 0.829\n",
      " 0.999  0.8576 0.8806 0.9515 1.     1.     1.     0.9252 0.9999 0.9999\n",
      " 0.9998 0.9978 0.905  0.9927 0.9954 0.9999 0.6125 0.7908 1.     0.8796\n",
      " 1.     0.9999 0.634  0.9989 1.     1.     1.     0.9999 0.9337 0.9978\n",
      " 1.     0.7754 0.9998 1.     1.     0.997  0.9959 0.9079 1.     0.9964]\n",
      "Condition _go_baseline: Found 0 significant clusters out of 60 total clusters.\n",
      "Processing Y group, _MAIN task, _go stage, _adaptation block...\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub01 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub07 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub10 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub11 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub22 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub24 complete.\n",
      "Found 15 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub26 complete.\n",
      "Found 17 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub29 complete.\n",
      "Found 9 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub32 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub33 complete.\n",
      "Found 2 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub38 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub43 complete.\n",
      "Found 1 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub52 complete.\n",
      "Found 6 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub58 complete.\n",
      "Found 3 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub59 complete.\n",
      "Found 26 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub61 complete.\n",
      "Found 5 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub63 complete.\n",
      "Found 12 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub64 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub66 complete.\n",
      "Found 13 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub67 complete.\n",
      "Found 7 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub71 complete.\n",
      "Found 10 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub76 complete.\n",
      "Found 8 vertices with NaN values.\n",
      "NaN imputation for s1_pac_sub77 complete.\n",
      "PAC array shape: (23, 5124, 20, 20)\n",
      "z-scored PAC array shape: (23, 5124, 20, 20)\n",
      "stat_fun(H1): min=-5.723391060710662 max=2.76290128321182\n",
      "Running initial clustering …\n",
      "Found 99 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f2c1ab367e4183996a59dd44346218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-down-in-jumps iteration #1 found 0 clusters to exclude from subsequent iterations\n",
      "t_thresh = 2.818756060596369\n",
      "T_obs_mean = -0.22435349356053857\n",
      "cluster_p_values = [0.9999 0.8681 0.0722 0.4712 0.5572 0.8751 0.5994 0.4936 0.9977 0.9999\n",
      " 0.477  0.9665 0.9945 0.8064 0.9933 0.9988 0.9987 0.6347 1.     1.\n",
      " 1.     0.9949 0.4341 0.9984 0.9129 0.9205 0.9032 0.8256 0.9785 0.9967\n",
      " 0.9973 0.9358 0.9989 0.9546 0.9999 0.9976 0.7807 0.9052 0.7734 0.9994\n",
      " 0.8735 0.9988 0.9987 0.9299 0.8621 0.9999 0.9996 0.8493 0.9999 0.7787\n",
      " 0.9869 0.5871 0.2554 0.0946 0.9998 0.9823 0.0548 0.9999 0.994  0.9881\n",
      " 0.4545 0.7787 0.9966 1.     0.9675 0.9804 0.8832 0.5279 0.9412 0.945\n",
      " 0.987  0.2929 0.4591 0.9823 0.9179 0.9999 0.8323 0.4265 0.7853 0.9996\n",
      " 0.994  0.6222 0.7794 0.9984 0.9997 0.9707 0.9716 0.7758 0.9356 0.749\n",
      " 0.9862 1.     0.8202 0.5237 0.9746 0.9989 0.9989 0.9249 0.9952]\n",
      "Condition _go_adaptation: Found 0 significant clusters out of 99 total clusters.\n"
     ]
    }
   ],
   "source": [
    "# Main script to process PAC data and run cluster-based permutation tests\n",
    "\n",
    "############### CREATE ADJACENCY MATRIX FOR STATISTICAL TEST #############\n",
    "# find_ch_adjacency first attempts to find an existing \"neighbor\"\n",
    "# (adjacency) file for given sensor layout.\n",
    "# If such a file doesn't exist, an adjacency matrix is computed on the fly,\n",
    "# using Delaunay triangulations.\n",
    "src_fname = 'D:\\\\BonoKat\\\\research project\\\\# study 1\\\\mri_data\\\\fs_output\\\\freesurfer\\\\sub_dir\\\\Y\\\\fsaverage_bem\\\\bem\\\\fsaverage-ico4-src.fif'\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "# src.plot(subjects_dir='D:\\\\BonoKat\\\\research project\\\\# study 1\\\\mri_data\\\\fs_output\\\\freesurfer\\\\sub_dir\\\\Y')\n",
    "source_adjacency = mne.spatial_src_adjacency(src)\n",
    "adj = source_adjacency.tocsr()  # Ensure adjacency is CSR format for fast indexing\n",
    "print('adjacency shape:', source_adjacency.shape)\n",
    "\n",
    "\n",
    "############## PROCESS PAC DATA FOR EACH GROUP AND TASK STAGE #############\n",
    "for group in groups:\n",
    "    group_save_path = os.path.join(eeg_data_dir, f'{group} group')\n",
    "    pac_stats_save_path = os.path.join(group_save_path, 'source_pac_stats')\n",
    "    check_paths(pac_stats_save_path)\n",
    "    \n",
    "    # # Create directories for saving figures\n",
    "    # fig_group_path = os.path.join(pac_stats_save_path, 'figs')\n",
    "    # fig_group_save_path = os.path.join(fig_group_path, group)\n",
    "    # fig_task_save_path = os.path.join(fig_group_path, group, task)\n",
    "    # check_paths(fig_group_path, fig_group_save_path, fig_task_save_path)\n",
    "\n",
    "    subs = os.listdir(os.path.join(eeg_data_dir, group))\n",
    "\n",
    "    for task_stage in task_stages: # [task_stages[0]]\n",
    "        for block_name in block_names: # [block_names[0]]\n",
    "\n",
    "            print(f'Processing {group} group, {task} task, {task_stage} stage, {block_name} block...')\n",
    "\n",
    "            ############# STACK PAC DATA OF INDIVIDUAL PARTICIPANTS #############\n",
    "\n",
    "            # Create a list to store the PAC data for each subject\n",
    "            pac_list = []\n",
    "            pac_zscore_list = []\n",
    "\n",
    "            for sub_name in subs:\n",
    "\n",
    "                sub_dir = os.path.join(eeg_data_dir, group, sub_name)\n",
    "                pac_dir = os.path.join(sub_dir, 'preproc', 'analysis', 'source', 'PAC')\n",
    "\n",
    "                # Load PAC data\n",
    "                pac = np.load(os.path.join(pac_dir, f\"PAC_MI_SOURCE_{sub_name[-5:]}{task}{task_stage}{block_name}.npy\"))\n",
    "                pac_t = np.transpose(pac, (1, 0, 2))\n",
    "\n",
    "                ### NAN Imputation for PAC Matrices ###\n",
    "                pac_imputed = pac_t.copy()\n",
    "\n",
    "                # Step 1: Detect vertices with any NaN in their 20×20 PAC\n",
    "                nan_mask = np.isnan(pac_t).any(axis=(1, 2))  # shape (5124,)\n",
    "\n",
    "                if nan_mask.any() == True:\n",
    "                    print(f\"Found {nan_mask.sum()} vertices with NaN values.\")\n",
    "\n",
    "                    # Step 2: Impute NaNs from neighbors\n",
    "                    for vtx in np.where(nan_mask)[0]:\n",
    "                        neighbors = adj[[vtx]].indices\n",
    "                        valid_neighbors = [n for n in neighbors if not nan_mask[n]]\n",
    "                        # Average PAC matrices from neighbors\n",
    "                        pac_imputed[vtx] = np.nanmean(pac_t[valid_neighbors], axis=0)\n",
    "\n",
    "                    print(f\"NaN imputation for {sub_name} complete.\")\n",
    "\n",
    "                pac_list.append(pac_imputed)\n",
    "                pac_zscore_list.append(zscore(pac_imputed, axis=0, nan_policy='omit')) # 'omit' ignores NaN values in the z-score calculation\n",
    "\n",
    "            # Stack them along a new first axis (subject axis)\n",
    "            pac_all = np.stack(pac_list, axis=0)\n",
    "\n",
    "            # Z-score the PAC data across subjects\n",
    "            pac_zscore_all = np.stack(pac_zscore_list, axis=0)\n",
    "            # pac_zscore_all = zscore(pac_all, axis=1, nan_policy='omit') # another way to zscore the data\n",
    "            print('PAC array shape:', pac_all.shape) # subs x electrodes x ph_freqs x amp_freqs\n",
    "            print('z-scored PAC array shape:', pac_zscore_all.shape)\n",
    "\n",
    "            # Averafe z-scored PAC over phase and amplitude frequencies\n",
    "            # pac_zscore_all_ave = np.mean(pac_zscore_all, axis=(2, 3)) # (23, 5124) subs x electrodes\n",
    "            # pac_zscore_all_med = np.median(pac_zscore_all, axis=(2, 3)) # produces more significant clusters\n",
    "\n",
    "            ### Global normalization\n",
    "            pac_all_ave = np.mean(pac_all, axis=(2, 3))\n",
    "            pac_zscore_all_ave = (pac_all_ave - np.nanmean(pac_all_ave)) / np.nanstd(pac_all_ave)\n",
    "            ###\n",
    "\n",
    "            # # Save the PAC data\n",
    "            np.save(os.path.join(pac_stats_save_path, f\"PAC_MI_SOURCE_{group}{task}{task_stage}{block_name}_ZSCORE_freqs_ave.npy\"), pac_zscore_all_ave)\n",
    "\n",
    "            # # ############# PLOT AND SAVE Z-SCORED PAC AVERAGED ACROSS PARTICIPANTS #############\n",
    "            # pac_plot, ax1 = plot_rect_topo_from_epochs(np.mean(pac_zscore_all_ave, axis=(0)), epochs.info,\n",
    "            #                                         title=f'{group}{task}{task_stage}{block_name}: Averaged z-scored PAC MI',\n",
    "            #                                         cmap='PiYG', vmin=-0.5, vmax=0.5)\n",
    "            # plt.savefig(os.path.join(fig_task_save_path, f\"pac_mi_{group}{task}{task_stage}{block_name}_PAC_MI_AVE_TOPO.png\"), dpi=300)\n",
    "\n",
    "            # pac_ave_plot, ax2 = plot_matrix_topo_from_epochs(np.mean(pac_zscore_all, axis=(0)), epochs.info,\n",
    "            #                                                 title=f'{group}{task}{task_stage}{block_name}: z-scored PAC MI',\n",
    "            #                                                 cmap='PiYG', vmin=-0.5, vmax=0.5)\n",
    "            # plt.savefig(os.path.join(fig_task_save_path, f\"pac_mi_{group}{task}{task_stage}{block_name}_PAC_MI_TOPO.png\"), dpi=300)\n",
    "\n",
    "\n",
    "            ############# RUN CLUSTER-BASED PERMUTATION TEST #############\n",
    "\n",
    "            tail = 0 # two-tailed test\n",
    "\n",
    "            # Set the threshold for including data bins in clusters with t-value corresponding to p=0.01\n",
    "            # Because we conduct a two-tailed test, we divide the p-value by 2 (which means we're making use of both tails of the distribution).\n",
    "            # As the degrees of freedom, we specify the number of observations (here: subjects) minus 1.\n",
    "            # Finally, we subtract 0.01 / 2 from 1, to get the critical t-value on the right tail\n",
    "            p_threshold = 0.01\n",
    "            degrees_of_freedom = pac_all.shape[0] - 1\n",
    "            t_thresh = scipy.stats.t.ppf(1 - p_threshold / 2, df=degrees_of_freedom)\n",
    "\n",
    "            #!\n",
    "            # threshold_tfce = dict(start=0, step=0.2) # Threshold-free cluster enhancement (TFCE) - more conservative, similar results\n",
    "\n",
    "            # Set the number of permutations\n",
    "            n_permutations = 10000\n",
    "\n",
    "            # Run the analysis\n",
    "            T_obs, clusters, cluster_p_values, H0 = permutation_cluster_1samp_test(\n",
    "                pac_zscore_all_ave,\n",
    "                n_permutations=n_permutations,\n",
    "                threshold=t_thresh, # None - default threshold based on t-distribution\n",
    "                tail=tail,\n",
    "                adjacency=adj,\n",
    "                step_down_p=0.05,  # step-down p-value correction instead of max stats\n",
    "                out_type=\"mask\",\n",
    "                max_step=1,\n",
    "                n_jobs=-1 # to use all available CPU cores\n",
    "            )\n",
    "\n",
    "            # # Save the results\n",
    "            np.save(os.path.join(pac_stats_save_path, f\"PAC_MI_SOURCE_{group}{task}{task_stage}{block_name}_freqs_ave_T_obs.npy\"), T_obs)\n",
    "            np.save(os.path.join(pac_stats_save_path, f\"PAC_MI_SOURCE_{group}{task}{task_stage}{block_name}_freqs_ave_clusters.npy\"), np.array(clusters, dtype=object))\n",
    "            np.save(os.path.join(pac_stats_save_path, f\"PAC_MI_SOURCE_{group}{task}{task_stage}{block_name}_freqs_ave_cluster_p_values.npy\"), cluster_p_values)\n",
    "            np.save(os.path.join(pac_stats_save_path, f\"PAC_MI_SOURCE_{group}{task}{task_stage}{block_name}_freqs_ave_H0.npy\"), H0)\n",
    "\n",
    "            # SANITY CHECKS\n",
    "            print(f't_thresh = {t_thresh}')\n",
    "            print(f'T_obs_mean = {T_obs.mean()}')\n",
    "            print(f'cluster_p_values = {cluster_p_values}')\n",
    "\n",
    "            alpha = 0.05  # significance threshold\n",
    "            significant_clusters = [i for i, p in enumerate(cluster_p_values) if p < alpha]\n",
    "            print(f\"Condition {task_stage}{block_name}: Found {len(significant_clusters)} significant clusters out of {len(cluster_p_values)} total clusters.\")\n",
    "\n",
    "\n",
    "            # ####### PLOT THE RESULTS #######\n",
    "            # plot_significant_topomap(T_obs, clusters, cluster_p_values, epochs.info, group=group, task=task, task_stage=task_stage, block_name=block_name)\n",
    "            # plt.savefig(os.path.join(fig_task_save_path, f\"pac_cluster_stats_{group}{task}{task_stage}{block_name}_freq_ave_TOPO.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c85475",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7c4b7",
   "metadata": {},
   "source": [
    "Dirty field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d95f9",
   "metadata": {},
   "source": [
    "PLOTTING PAC DISTRIBUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a92d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global distribution of all PAC values across all subjects and vertices\n",
    "# Flatten all PAC values into a 1D array\n",
    "all_pac_values = pac_all.flatten()\n",
    "\n",
    "# Remove NaNs if present\n",
    "all_pac_values = all_pac_values[~np.isnan(all_pac_values)]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(all_pac_values, bins=100, color='skyblue', edgecolor='k')\n",
    "plt.title(\"Distribution of all PAC z-scores\")\n",
    "plt.xlabel(\"PAC z-score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution per subject (e.g., mean PAC per subject)\n",
    "# Mean PAC per subject, averaged over all space and frequencies\n",
    "subject_means = np.nanmean(pac_all, axis=(1, 2, 3))  # shape (23,)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(subject_means, bins=15, color='coral', edgecolor='k')\n",
    "plt.title(\"Mean PAC z-scores per subject\")\n",
    "plt.xlabel(\"Mean PAC z-score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "046a3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot: per-subject PAC distributions\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample 5000 random PAC values per subject (if needed for speed)\n",
    "np.random.seed(42)\n",
    "subset = [pac_all[i].flatten() for i in range(23)]\n",
    "subset = [x[~np.isnan(x)] for x in subset]\n",
    "subset = [np.random.choice(x, 5000, replace=False) if len(x) > 5000 else x for x in subset]\n",
    "\n",
    "# Create DataFrame for seaborn\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    'PAC': np.concatenate(subset),\n",
    "    'Subject': np.concatenate([[f'Subj {i+1}'] * len(x) for i, x in enumerate(subset)])\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=df, x='Subject', y='PAC', inner='box')\n",
    "plt.title(\"PAC value distribution per subject\")\n",
    "plt.ylabel(\"PAC z-score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "574b9ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(5.687941503192036e-07)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(pac_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
